{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf76c30-4cfb-42d2-ab59-8ff96dfb74e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "815d6199-17b2-4e8f-aede-60ae3c35d4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home1/zzhang201@kgi.edu/GAN/zzGAN/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29b016c6-6cd8-4b00-b56d-268d1abbd146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 05:11:09.090905: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-06 05:11:09.122778: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# import tensorflow_gan as tfgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c01e5ef2-45e5-4008-a99a-c108ba64dfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.16.1\n",
      "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"GPUs:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f7a9d9f-d845-44f6-83e2-6bfb7996d386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c767d0e-858f-4169-9d34-6951dbe77a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/zzhang201@kgi.edu/.conda/envs/tf2_16/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33435abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import datetime\n",
    "import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6785f0f-c59d-4899-b738-2ee049d2ecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import gan.documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2052766",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gan.models import get_model, get_specific_hooks\n",
    "from gan.parameters import get_flags\n",
    "from gan.documentation import setup_logdir, get_properties\n",
    "from gan.documentation import print_run_meta_data, add_model_metadata\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.mixed_precision import LossScaleOptimizer\n",
    "# Enable global mixed precision policy\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "from protein.quality_gates import quality_losses, anarci_quality_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0040c371-8a1c-4349-a3f1-b8bcc2b2ec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeFlags:\n",
    "    # General model setup\n",
    "    model_type = 'wgan'\n",
    "    architecture = 'gumbel'\n",
    "    batch_size = 64\n",
    "    z_dim = 128\n",
    "    gf_dim = 64\n",
    "    df_dim = 64\n",
    "    dim = gf_dim\n",
    "    attn_pos = 2\n",
    "\n",
    "    # Kernel and dilation config\n",
    "    kernel_height = 3\n",
    "    kernel_width = 3\n",
    "    dilation_rate = 2\n",
    "    pooling = 'conv'\n",
    "\n",
    "    # Optional: logging / save frequency\n",
    "    name = 'trial2'\n",
    "    steps = 20000\n",
    "    save_summary_steps = 1000\n",
    "    save_checkpoint_sec = 5000\n",
    "\n",
    "    # Optimizer settings\n",
    "    generator_learning_rate = 1e-4\n",
    "    discriminator_learning_rate = 1e-4\n",
    "    beta1 = 0.5\n",
    "    beta2 = 0.9\n",
    "\n",
    "    # Dataset & file structure\n",
    "    dataset = 'zz'\n",
    "    seq_length = 160\n",
    "    logdir = '/home1/zzhang201@kgi.edu/GAN/zzGAN/logs/zz'\n",
    "    # properties_file = 'properties.json'\n",
    "\n",
    "    # Misc\n",
    "    seed = 970713\n",
    "    resume_from = None\n",
    "    # finetune_from = '/home1/zzhang201@kgi.edu/GAN/zzGAN/logs/zz/multiG_attn_adjusted_embed/20250724-032823/'\n",
    "    multid_schedule = 20000\n",
    "    d_step = 3\n",
    "    shuffle_buffer_size = 1000\n",
    "    label_noise_level = 0.0\n",
    "    noise_level = 0.0\n",
    "\n",
    "    def flag_values_dict(self):\n",
    "        return {k: getattr(self, k) for k in dir(self)\n",
    "                if not k.startswith(\"__\") and not callable(getattr(self, k))}\n",
    "FLAGS = FakeFlags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "339bfacc-054f-48b8-b491-c4bbc50bb11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 05:11:21.472899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43622 MB memory:  -> device: 0, name: NVIDIA L40S, pci bus id: 0000:21:00.0, compute capability: 8.9\n",
      "2025-10-06 05:11:21.473436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 43622 MB memory:  -> device: 1, name: NVIDIA L40S, pci bus id: 0000:e1:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "random.seed(FLAGS.seed)\n",
    "np.random.seed(FLAGS.seed)\n",
    "tf.random.set_seed(FLAGS.seed)\n",
    "# Fixed latent for eval snapshots so runs are comparable\n",
    "Z_EVAL = tf.random.stateless_normal([128, FLAGS.z_dim], seed=(970, 713))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "defe8e17-ad68-48a9-9124-8c8968a7b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_flags(flags, log_dir, filename=\"flags.json\"):\n",
    "    \"\"\"Save FLAGS to a JSON file in the log directory.\"\"\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    file_path = os.path.join(log_dir, filename)\n",
    "\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(flags.flag_values_dict(), f, indent=4)\n",
    "    \n",
    "    print(f\"[FLAGS] Saved flags to {file_path}\")\n",
    "\n",
    "def load_flags(log_dir, filename=\"flags.json\"):\n",
    "    \"\"\"Load FLAGS from JSON file and return as a FakeFlags object.\"\"\"\n",
    "    file_path = os.path.join(log_dir, filename)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"[FLAGS] No flags file found at {file_path}\")\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        flag_dict = json.load(f)\n",
    "\n",
    "    # Create a new FakeFlags object and set attributes\n",
    "    loaded_flags = FakeFlags()\n",
    "    for k, v in flag_dict.items():\n",
    "        setattr(loaded_flags, k, v)\n",
    "\n",
    "    print(f\"[FLAGS] Loaded flags from {file_path}\")\n",
    "    return loaded_flags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01d532a0-2ebb-4502-8e0b-51d512fd2b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/zzhang201@kgi.edu/.conda/envs/tf2_16/lib/python3.9/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n",
      "2025-10-06 05:11:21.801446: E tensorflow/core/util/util.cc:131] oneDNN supports DT_HALF only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n",
      "2025-10-06 05:11:22.608925: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing from scratch.\n",
      "[FLAGS] Saved flags to /home1/zzhang201@kgi.edu/GAN/zzGAN/logs/zz/trial2/20251006-051121/summaries/flags.json\n"
     ]
    }
   ],
   "source": [
    "import os, json, datetime, tensorflow as tf\n",
    "from protein.ema import EMA\n",
    "# 0) Make run dirs first (RUN_DIR, CKPT_DIR, SUMM_DIR, ATTN_DIR, etc.)\n",
    "timestamp   = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_DIR     = os.path.join(FLAGS.logdir, FLAGS.name, timestamp)\n",
    "CKPT_DIR    = os.path.join(RUN_DIR, \"checkpoints\")\n",
    "SUMM_DIR    = os.path.join(RUN_DIR, \"summaries\")\n",
    "ATTN_DIR    = os.path.join(RUN_DIR, \"attn_scores\")\n",
    "GENS_DIR    = os.path.join(RUN_DIR, \"generated\")\n",
    "\n",
    "for d in (CKPT_DIR, SUMM_DIR, ATTN_DIR, GENS_DIR):\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# 1) Build models/opts (objects only; no variables yet)\n",
    "noise = tf.random.normal([FLAGS.batch_size, FLAGS.z_dim], dtype=tf.float32)\n",
    "model = get_model(FLAGS, RUN_DIR, noise)\n",
    "g_model, d_model = model.g_model, model.d_model\n",
    "g_opt, d_opt     = model.g_optim, model.d_optim\n",
    "\n",
    "# 2) Global step (int32 to match old ckpts) and attach to G\n",
    "global_step = tf.Variable(0, dtype=tf.int32, name=\"global_step\")\n",
    "setattr(g_model, \"global_step\", global_step)\n",
    "\n",
    "# 3) WARM-UP once in fp32 to create ALL variables eagerly (prevents trace/deferred issues)\n",
    "_ = g_model(tf.random.normal([1, FLAGS.z_dim], dtype=tf.float32), training=False, return_hard=False)\n",
    "SEQ_LEN = getattr(FLAGS, \"seq_len\", 160)\n",
    "VOCAB   = getattr(FLAGS, \"vocab_size\", 21)\n",
    "_ = d_model(tf.random.normal([1, 1, SEQ_LEN, VOCAB], dtype=tf.float32), training=False)\n",
    "\n",
    "# 4) Create EMA object and **build it now** (so shadows exist for restore)\n",
    "ema = EMA(decay=getattr(FLAGS, \"ema_decay\", 0.999))\n",
    "ema.build(g_model)   # shadows initialized from current g_model weights (fp32)\n",
    "\n",
    "# 5) Create a checkpoint that includes models, optimizers, step, and EMA\n",
    "# We now explicitly pass the generator's list of variables\n",
    "ckpt = tf.train.Checkpoint(\n",
    "    generator_variables=g_model.variables,  # The FIX: Pass the variable list directly\n",
    "    discriminator=d_model,\n",
    "    g_optimizer=g_opt,\n",
    "    d_optimizer=d_opt,\n",
    "    step=global_step,\n",
    "    ema=ema\n",
    ")\n",
    "\n",
    "# 6) Restore (if any). Use a *temporary manager* pointed at the SOURCE dir.\n",
    "RESUME_FROM   = getattr(FLAGS, \"resume_from\", None)\n",
    "FINETUNE_FROM = getattr(FLAGS, \"finetune_from\", None)\n",
    "\n",
    "def _restore_from(src_dir, label):\n",
    "    src_mgr = tf.train.CheckpointManager(ckpt, os.path.join(src_dir, \"checkpoints\"), max_to_keep=5)\n",
    "    if not src_mgr.latest_checkpoint:\n",
    "        raise FileNotFoundError(f\"No checkpoint found in {src_dir}/checkpoints\")\n",
    "    status = ckpt.restore(src_mgr.latest_checkpoint)\n",
    "    status.expect_partial()  # OK: SN u-vectors & (possibly) EMA may differ\n",
    "    print(f\"[{label}] restored:\", src_mgr.latest_checkpoint)\n",
    "\n",
    "if RESUME_FROM:\n",
    "    _restore_from(RESUME_FROM, \"RESUME\")      # same run, same objective → restores opts + EMA if present\n",
    "elif FINETUNE_FROM:\n",
    "    _restore_from(FINETUNE_FROM, \"FINETUNE\")  # see weights-only variant below if you don't want optimizer state\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")\n",
    "\n",
    "# 7) Destination manager for THIS run (saves to your new RUN_DIR)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, CKPT_DIR, max_to_keep=5)\n",
    "\n",
    "# 8) Summary writer\n",
    "summary_writer = tf.summary.create_file_writer(SUMM_DIR)\n",
    "\n",
    "# 9) Save flags, expose paths\n",
    "save_flags(FLAGS, SUMM_DIR)\n",
    "attn_dir = ATTN_DIR\n",
    "log_dir  = RUN_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25eb59dd-b1e0-4313-a26c-6541cca3debe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial checkpoint saved to: /home1/zzhang201@kgi.edu/GAN/zzGAN/logs/zz/trial2/20251006-051121/checkpoints/ckpt-1\n",
      "All checkpoint keys:\n",
      "_CHECKPOINTABLE_OBJECT_GRAPH\n",
      "d_optimizer/_iterations/.ATTRIBUTES/VARIABLE_VALUE\n",
      "d_optimizer/_learning_rate/.ATTRIBUTES/VARIABLE_VALUE\n",
      "d_optimizer/inner_optimizer/_iterations/.ATTRIBUTES/VARIABLE_VALUE\n",
      "d_optimizer/inner_optimizer/_learning_rate/.ATTRIBUTES/VARIABLE_VALUE\n",
      "discriminator/conv1/_kernel/.ATTRIBUTES/VARIABLE_VALUE\n",
      "discriminator/conv1/bias/.ATTRIBUTES/VARIABLE_VALUE\n",
      "discriminator/conv2/_kernel/.ATTRIBUTES/VARIABLE_VALUE\n",
      "discriminator/conv2/bias/.ATTRIBUTES/VARIABLE_VALUE\n",
      "discriminator/conv3/_kernel/.ATTRIBUTES/VARIABLE_VALUE\n",
      "discriminator/conv3/bias/.ATTRIBUTES/VARIABLE_VALUE\n",
      "discriminator/dense/_kernel/.ATTRIBUTES/VARIABLE_VALUE\n",
      "discriminator/dense/bias/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/0/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/1/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/10/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/11/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/12/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/13/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/14/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/15/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/16/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/17/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/18/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/19/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/2/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/20/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/21/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/22/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/23/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/24/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/25/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/26/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/27/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/28/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/29/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/3/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/4/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/5/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/6/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/7/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/8/.ATTRIBUTES/VARIABLE_VALUE\n",
      "ema/shadow_vars/9/.ATTRIBUTES/VARIABLE_VALUE\n",
      "g_optimizer/_iterations/.ATTRIBUTES/VARIABLE_VALUE\n",
      "g_optimizer/_learning_rate/.ATTRIBUTES/VARIABLE_VALUE\n",
      "g_optimizer/inner_optimizer/_iterations/.ATTRIBUTES/VARIABLE_VALUE\n",
      "g_optimizer/inner_optimizer/_learning_rate/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/0/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/1/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/10/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/11/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/12/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/13/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/14/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/15/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/16/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/17/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/18/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/19/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/2/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/20/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/21/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/22/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/23/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/24/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/25/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/26/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/27/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/28/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/29/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/3/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/30/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/31/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/32/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/33/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/34/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/35/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/36/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/37/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/38/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/39/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/4/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/40/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/41/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/42/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/43/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/44/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/45/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/46/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/47/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/48/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/5/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/6/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/7/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/8/.ATTRIBUTES/VARIABLE_VALUE\n",
      "generator_variables/9/.ATTRIBUTES/VARIABLE_VALUE\n",
      "save_counter/.ATTRIBUTES/VARIABLE_VALUE\n",
      "step/.ATTRIBUTES/VARIABLE_VALUE\n",
      "Attn-related keys:\n"
     ]
    }
   ],
   "source": [
    "# Save an initial checkpoint (at step 0) and inspect variables\n",
    "initial_ckpt_path = ckpt_manager.save()\n",
    "print(f\"Initial checkpoint saved to: {initial_ckpt_path}\")\n",
    "# List all variable names in the saved checkpoint\n",
    "keys = [n for n, _ in tf.train.list_variables(initial_ckpt_path)]\n",
    "print(\"All checkpoint keys:\")\n",
    "for k in sorted(keys):\n",
    "    print(k)\n",
    "# Optionally filter and print attn-related keys\n",
    "attn_keys = [k for k in keys if k.startswith('generator/attn/')]\n",
    "print(\"Attn-related keys:\")\n",
    "for k in sorted(attn_keys):\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "855a778b-e9af-484d-b972-14e5ca953f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_example(example_proto, seq_len=160, vocab_size=21):\n",
    "    feature_description = {\n",
    "        \"sequence\": tf.io.VarLenFeature(tf.int64),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64),  # parsed but unused\n",
    "    }\n",
    "    parsed = tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "    sequence = tf.sparse.to_dense(parsed[\"sequence\"])\n",
    "    sequence = tf.cast(sequence, tf.int32)\n",
    "\n",
    "    # Clip or pad to fixed length\n",
    "    sequence = sequence[:seq_len]\n",
    "    paddings = [[0, tf.maximum(0, seq_len - tf.shape(sequence)[0])]]\n",
    "    sequence = tf.pad(sequence, paddings)\n",
    "\n",
    "    # One-hot encode → [seq_len, vocab_size] float32\n",
    "    one_hot = tf.one_hot(sequence, vocab_size)\n",
    "    return one_hot\n",
    "\n",
    "def load_tfrecord_dataset(\n",
    "    tfrecord_dir,\n",
    "    batch_size=8,\n",
    "    seq_len=160,\n",
    "    vocab_size=21,\n",
    "    shuffle_buffer=10000,\n",
    "    seed=42,\n",
    "    deterministic=True,\n",
    "    cycle_length=4,\n",
    "    drop_remainder=True,\n",
    "):\n",
    "    # Stable file order (DO NOT shuffle here)\n",
    "    files = sorted(tf.io.gfile.glob(os.path.join(tfrecord_dir, \"*.tfrecords\")))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No .tfrecords under {tfrecord_dir}\")\n",
    "\n",
    "    ds_files = tf.data.Dataset.from_tensor_slices(files)\n",
    "\n",
    "    # Deterministic interleave over files\n",
    "    ds = ds_files.interleave(\n",
    "        lambda p: tf.data.TFRecordDataset(p),\n",
    "        cycle_length=cycle_length,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        deterministic=deterministic,\n",
    "    )\n",
    "\n",
    "    ds = ds.map(\n",
    "        lambda ex: parse_example(ex, seq_len, vocab_size),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    )\n",
    "\n",
    "    # Seeded shuffle; NO reshuffle each epoch → same order every run\n",
    "    if shuffle_buffer and shuffle_buffer > 0:\n",
    "        ds = ds.shuffle(\n",
    "            buffer_size=shuffle_buffer,\n",
    "            seed=seed,\n",
    "            reshuffle_each_iteration=False,\n",
    "        )\n",
    "\n",
    "    # Infinite stream with fixed batch shapes\n",
    "    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n",
    "    ds = ds.repeat()\n",
    "\n",
    "    # Enforce deterministic behavior at the pipeline level if requested\n",
    "    opts = tf.data.Options()\n",
    "    opts.deterministic = deterministic\n",
    "    ds = ds.with_options(opts)\n",
    "\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62232c39-2f12-42bd-97ec-7bf5e22feb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 160, 21)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_tfrecord_dataset(\n",
    "    tfrecord_dir=\"/home1/zzhang201@kgi.edu/GAN/zzGAN/gan/data/zz/train\",\n",
    "    batch_size=FLAGS.batch_size,\n",
    "    seq_len=160,\n",
    "    seed=FLAGS.seed,\n",
    "    deterministic=True,  # flip to False if you want max throughput\n",
    ")\n",
    "\n",
    "batch = next(iter(dataset))\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b55e219-dcf8-409f-b93c-ec1c4c397f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_discriminator_step(generator, discriminator, d_optimizer, real_batch, noise_dim, lambda_gp=10.0):\n",
    "    bsz  = tf.shape(real_batch)[0]\n",
    "    real = tf.cast(tf.expand_dims(real_batch, 1), tf.float32)     # [B,1,L,V]\n",
    "\n",
    "    z = tf.random.normal([bsz, noise_dim], dtype=tf.float32)\n",
    "    fake_soft = generator(z, training=True, return_hard=False)    # differentiable\n",
    "    fake = tf.stop_gradient(tf.cast(fake_soft, tf.float32))       # isolate D update\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as disc_tape:\n",
    "        real_logits = discriminator(real, training=True)\n",
    "        fake_logits = discriminator(fake, training=True)\n",
    "\n",
    "        d_wgan = tf.reduce_mean(fake_logits) - tf.reduce_mean(real_logits)\n",
    "\n",
    "        # GP @ interpolated points\n",
    "        alpha = tf.random.uniform([bsz, 1, 1, 1], 0.0, 1.0, dtype=tf.float32)\n",
    "        inter = real + alpha * (fake - real)  # equivalent to alpha*real+(1-alpha)*fake\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(inter)\n",
    "            inter_out = discriminator(inter, training=True)\n",
    "        inter_grads = gp_tape.gradient(inter_out, inter)\n",
    "        grad_norm   = tf.sqrt(tf.reduce_sum(tf.square(inter_grads), axis=[1,2,3]) + 1e-12)\n",
    "        gp          = tf.reduce_mean(tf.square(grad_norm - 1.0))\n",
    "\n",
    "        d_loss = d_wgan + lambda_gp * gp\n",
    "\n",
    "    d_vars  = discriminator.trainable_variables\n",
    "    d_grads = disc_tape.gradient(d_loss, d_vars)\n",
    "    d_missing = _count_missing_grads_t(d_grads)\n",
    "\n",
    "    # Filter None grads\n",
    "    pairs = [(g, v) for g, v in zip(d_grads, d_vars) if g is not None]\n",
    "    d_optimizer.apply_gradients(pairs)\n",
    "\n",
    "    d_grad_norm = tf.linalg.global_norm([g for g, _ in pairs]) if pairs else tf.constant(0., tf.float32)\n",
    "\n",
    "    # Also return means of logits to see separation\n",
    "    real_logit_mean = tf.reduce_mean(real_logits)\n",
    "    fake_logit_mean = tf.reduce_mean(fake_logits)\n",
    "\n",
    "    return d_loss, d_grad_norm, d_wgan, gp, d_missing, real_logit_mean, fake_logit_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2a8d061-a7e7-48f3-b7ad-c13bce2a65f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_generator_step(generator, discriminator, g_optimizer, noise_dim, entropy_weight=0.01):\n",
    "    bsz = FLAGS.batch_size\n",
    "    z = tf.random.normal([bsz, noise_dim], dtype=tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        # Run G once to fill last_logits\n",
    "        _ = generator(z, training=True, return_hard=False)\n",
    "        logits = getattr(generator, \"last_logits\", None)\n",
    "\n",
    "        if logits is not None:\n",
    "            p = tf.nn.softmax(tf.cast(logits, tf.float32), axis=-1)  # [B,1,L,V]\n",
    "        else:\n",
    "            p = tf.cast(generator(z, training=True, return_hard=False), tf.float32)\n",
    "\n",
    "        fake_logits_D = discriminator(p, training=True)\n",
    "        g_adv = -tf.reduce_mean(fake_logits_D)\n",
    "\n",
    "        entropy_pos = -tf.reduce_sum(p * tf.math.log(tf.clip_by_value(p, 1e-8, 1.0)), axis=-1)  # [B,1,L]\n",
    "        entropy = tf.reduce_mean(entropy_pos)\n",
    "\n",
    "        g_loss = g_adv - entropy_weight * entropy\n",
    "\n",
    "    g_vars  = generator.trainable_variables\n",
    "    g_grads = gen_tape.gradient(g_loss, g_vars)\n",
    "    g_missing = _count_missing_grads_t(g_grads)\n",
    "\n",
    "    pairs = [(g, v) for g, v in zip(g_grads, g_vars) if g is not None]\n",
    "    g_optimizer.apply_gradients(pairs)\n",
    "\n",
    "    # EMA after G update (if you use EMA)\n",
    "    ema.update(generator)\n",
    "\n",
    "    g_grad_norm = tf.linalg.global_norm([g for g, _ in pairs]) if pairs else tf.constant(0., tf.float32)\n",
    "    return g_loss, g_grad_norm, g_missing, entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aff8a24c-3ee2-468a-bf24-0774718b47f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Small manifest to catch accidental drift\\nimport json, platform\\nmanifest = {\\n    \"timestamp\": timestamp,\\n    \"mode\": \"resume\" if RESUME_FROM else (\"finetune\" if FINETUNE_FROM else \"fresh\"),\\n    \"resume_from\": RESUME_FROM,\\n    \"finetune_from\": FINETUNE_FROM,\\n    \"seed\": int(FLAGS.seed),\\n    \"z_dim\": int(FLAGS.z_dim),\\n    \"batch_size\": int(FLAGS.batch_size),\\n    \"ema\": bool(USE_EMA),\\n    \"ema_decay\": float(EMA_DECAY),\\n    \"lr_g\": float(getattr(g_opt.learning_rate, \"numpy\", lambda: g_opt.learning_rate)()),\\n    \"lr_d\": float(getattr(d_opt.learning_rate, \"numpy\", lambda: d_opt.learning_rate)()),\\n    \"python\": platform.python_version(),\\n    \"tensorflow\": tf.__version__,\\n}\\nwith open(os.path.join(run_dir, \"run_manifest.json\"), \"w\") as f:\\n    json.dump(manifest, f, indent=2)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Small manifest to catch accidental drift\n",
    "import json, platform\n",
    "manifest = {\n",
    "    \"timestamp\": timestamp,\n",
    "    \"mode\": \"resume\" if RESUME_FROM else (\"finetune\" if FINETUNE_FROM else \"fresh\"),\n",
    "    \"resume_from\": RESUME_FROM,\n",
    "    \"finetune_from\": FINETUNE_FROM,\n",
    "    \"seed\": int(FLAGS.seed),\n",
    "    \"z_dim\": int(FLAGS.z_dim),\n",
    "    \"batch_size\": int(FLAGS.batch_size),\n",
    "    \"ema\": bool(USE_EMA),\n",
    "    \"ema_decay\": float(EMA_DECAY),\n",
    "    \"lr_g\": float(getattr(g_opt.learning_rate, \"numpy\", lambda: g_opt.learning_rate)()),\n",
    "    \"lr_d\": float(getattr(d_opt.learning_rate, \"numpy\", lambda: d_opt.learning_rate)()),\n",
    "    \"python\": platform.python_version(),\n",
    "    \"tensorflow\": tf.__version__,\n",
    "}\n",
    "with open(os.path.join(run_dir, \"run_manifest.json\"), \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3d89b9a-199b-44e8-990c-ac7c986f9dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Where to put generated sequences for this run\n",
    "log_dir = Path(RUN_DIR)\n",
    "SEQ_DIR = log_dir / \"generated_sequences\" # keep alongside your \"summaries\"\n",
    "SEQ_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FASTA_PATH = SEQ_DIR / \"generated_sequences.fasta\"\n",
    "INDEX_PATH = SEQ_DIR / \"generated_sequences_index.csv\"\n",
    "\n",
    "# Write CSV header once if file is new\n",
    "if not INDEX_PATH.exists():\n",
    "    with open(INDEX_PATH, \"w\", newline=\"\") as f:\n",
    "        csv.writer(f).writerow([\"step\",\"seq_idx\",\"length\"])\n",
    "        \n",
    "AA = set(\"ACDEFGHIKLMNPQRSTVWYXBZ-\")\n",
    "def sanitize_seq(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\", \"\", s).upper()\n",
    "    return \"\".join(ch for ch in s if ch in AA)\n",
    "\n",
    "def append_generated_sequences(step: int, sequences):\n",
    "    \"\"\"Append sequences to FASTA + index CSV. Header stores the step + idx.\"\"\"\n",
    "    with open(FASTA_PATH, \"a\") as ffa, open(INDEX_PATH, \"a\", newline=\"\") as fcsv:\n",
    "        w = csv.writer(fcsv)\n",
    "        for i, s in enumerate(sequences):\n",
    "            seq = sanitize_seq(s)\n",
    "            header = f\"step{step:06d}_idx{i}\"\n",
    "            ffa.write(f\">{header}\\n{seq}\\n\")\n",
    "            w.writerow([step, i, len(seq)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cd7b5f9-792f-4052-914c-08e5cd33d4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMINO_ACIDS = \"ACDEFGHIKLMNPQRSTVWY\"  # 20 residues\n",
    "AA_TO_IDX = {aa: i + 1 for i, aa in enumerate(AMINO_ACIDS)}\n",
    "PAD_IDX = 0\n",
    "VOCAB_SIZE = 21  # 0 (PAD) + 1–20\n",
    "\n",
    "MAX_LEN = 160\n",
    "\n",
    "def decode_sequence(onehot):\n",
    "    tokens = tf.argmax(onehot, axis=-1).numpy().flatten()\n",
    "    return ''.join([AMINO_ACIDS[i - 1] if i != 0 else '-' for i in tokens])\n",
    "\n",
    "def decode_batch(batch_tensor: tf.Tensor) -> list[str]:\n",
    "    \"\"\"\n",
    "    Decodes a batch of one-hot encoded tensors into amino acid sequences.\n",
    "\n",
    "    Supports input of shape:\n",
    "        - (B, 1, L, 21)\n",
    "        - (B, L, 21)\n",
    "\n",
    "    Returns:\n",
    "        List of decoded strings of length B\n",
    "    \"\"\"\n",
    "    if batch_tensor.shape.rank == 4:\n",
    "        batch_tensor = tf.squeeze(batch_tensor, axis=1)  # (B, L, 21)\n",
    "\n",
    "    return [decode_sequence(seq) for seq in batch_tensor]\n",
    "\n",
    "\n",
    "def encode_sequence(seq):\n",
    "    \"\"\"\n",
    "    Encode a sequence into one-hot with shape [1, MAX_LEN, 21]\n",
    "    \"\"\"\n",
    "    seq = seq.upper()\n",
    "    indices = [AA_TO_IDX.get(aa, PAD_IDX) for aa in seq[:MAX_LEN]]\n",
    "    if len(indices) < MAX_LEN:\n",
    "        indices += [PAD_IDX] * (MAX_LEN - len(indices))\n",
    "    one_hot = tf.one_hot(indices, depth=VOCAB_SIZE, dtype=tf.float32)  # [MAX_LEN, 21]\n",
    "    return tf.expand_dims(one_hot, axis=0)  # [1, MAX_LEN, 21]\n",
    "\n",
    "def encode_batch(sequences):\n",
    "    \"\"\"\n",
    "    Encode a batch of sequences -> [B, 1, MAX_LEN, 21]\n",
    "    \"\"\"\n",
    "    encoded = [encode_sequence(seq) for seq in sequences]\n",
    "    return tf.stack(encoded, axis=0)  # [B, 1, MAX_LEN, 21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a55a1e3-4770-40bf-a81c-8641cbc1e623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aa_distribution(batch):\n",
    "    \"\"\"\n",
    "    Compute normalized amino acid distribution from a one-hot batch,\n",
    "    excluding padding (index 0). Supports shape (B, 1, L, 21) or (B, L, 21).\n",
    "    \"\"\"\n",
    "    if batch.shape.rank == 4:\n",
    "        batch = tf.squeeze(batch, axis=1)  # → (B, L, 21)\n",
    "\n",
    "    batch = tf.cast(batch, tf.float32)\n",
    "    # Exclude padding channel (index 0)\n",
    "    batch = batch[..., 1:]  # → shape: (B, L, 20)\n",
    "\n",
    "    counts = tf.reduce_sum(batch, axis=[0, 1])  # → (20,)\n",
    "    total = tf.reduce_sum(counts)\n",
    "    prob = tf.where(total > 0, counts / total, tf.zeros_like(counts))\n",
    "    return prob.numpy()\n",
    "\n",
    "def js_divergence_per_position(real_batch, gen_batch):\n",
    "    \"\"\"\n",
    "    Computes JS divergence for each position along the sequence (excluding padding index).\n",
    "    Supports input shape (B, 1, L, 21) or (B, L, 21).\n",
    "    Returns: np.array of shape (L,) — JS divergence per position\n",
    "    \"\"\"\n",
    "    if real_batch.shape.rank == 4:\n",
    "        real_batch = tf.squeeze(real_batch, axis=1)\n",
    "    if gen_batch.shape.rank == 4:\n",
    "        gen_batch = tf.squeeze(gen_batch, axis=1)\n",
    "\n",
    "    real_batch = tf.cast(real_batch, tf.float32).numpy()\n",
    "    gen_batch = tf.cast(gen_batch, tf.float32).numpy()\n",
    "\n",
    "    L = real_batch.shape[1]\n",
    "    jsd_pos = []\n",
    "\n",
    "    for i in range(L):\n",
    "        p_real = np.mean(real_batch[:, i, 1:], axis=0)  # exclude padding (index 0)\n",
    "        p_gen = np.mean(gen_batch[:, i, 1:], axis=0)\n",
    "        p_real /= np.sum(p_real) + 1e-8\n",
    "        p_gen /= np.sum(p_gen) + 1e-8\n",
    "        jsd = jensenshannon(p_real, p_gen, base=2)\n",
    "        jsd_pos.append(jsd)\n",
    "    \n",
    "    return np.array(jsd_pos)\n",
    "\n",
    "def sequence_lengths(onehot_batch):\n",
    "    indices = tf.argmax(onehot_batch, axis=-1)\n",
    "    non_padding = tf.not_equal(indices, 0)\n",
    "    return tf.reduce_sum(tf.cast(non_padding, tf.int32), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a37eb7c-d89d-4304-b1ce-ba13017cbe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_hamming_distance(seq1, seq2):\n",
    "    \"\"\"Compute normalized Hamming distance between two equal-length strings.\"\"\"\n",
    "    assert len(seq1) == len(seq2), \"Sequences must be the same length\"\n",
    "    return sum(a != b for a, b in zip(seq1, seq2)) / len(seq1)\n",
    "\n",
    "def filter_diverse_sequences(sequences, threshold=0.05, max_count=None):\n",
    "    diverse = []\n",
    "    for seq in sequences:\n",
    "        if all(normalized_hamming_distance(seq, d) >= threshold for d in diverse):\n",
    "            diverse.append(seq)\n",
    "        if max_count is not None and len(diverse) >= max_count:\n",
    "            break\n",
    "    return diverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7ce38a1-383e-4f18-a76d-f00526421cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally pass amino acid sequence if you want labels\n",
    "def save_attention_png(attn, filename=\"attn_map.png\", aa_seq=None):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(attn, cmap='viridis')\n",
    "    plt.title(\"Attention Heatmap\")\n",
    "    if aa_seq:\n",
    "        plt.xticks(ticks=range(len(aa_seq)), labels=list(aa_seq), fontsize=6, rotation=90)\n",
    "        plt.yticks(ticks=range(len(aa_seq)), labels=list(aa_seq), fontsize=6)\n",
    "    else:\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02600cd7-8e4b-4e06-be87-b959ac54b7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_attention_to_disk(attn_scores, sequences, step, output_dir=\"attn_logs\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for i in range(attn_scores.shape[0]):\n",
    "        sample_path = os.path.join(output_dir, f\"sample_{i}_step_{step}.npz\")\n",
    "        np.savez_compressed(sample_path,\n",
    "                            attn=attn_scores[i].numpy(),  # shape [H, W, W]\n",
    "                            sequence=sequences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59b88631-e295-4874-b7f4-431081850047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_attention_to_tensorboard(attn_scores, sequences, step, summary_writer):\n",
    "    \"\"\"\n",
    "    attn_scores: Tensor [B, H, W, W]\n",
    "    sequences: list of strings\n",
    "    \"\"\"\n",
    "    B, H, W, _ = attn_scores.shape\n",
    "    attn_scores = attn_scores.numpy()  # Convert for plotting\n",
    "    with summary_writer.as_default():\n",
    "        for i in range(min(B, 2)):  # log up to 2 samples\n",
    "            for h in range(H):\n",
    "                fig, ax = plt.subplots(figsize=(6, 6), dpi=100)\n",
    "                attn = attn_scores[i, h]  # [W, W]\n",
    "                im = ax.imshow(attn, cmap='magma', vmin=0.0, vmax=1.0)\n",
    "\n",
    "                ax.set_title(f\"Sample {i}, Head {h}\")\n",
    "                ax.set_xlabel(\"Residue Index\")\n",
    "                ax.set_ylabel(\"Residue Index\")\n",
    "\n",
    "                # Show ticks every 20 residues for readability\n",
    "                tick_positions = np.arange(0, W, 20)\n",
    "                ax.set_xticks(tick_positions)\n",
    "                ax.set_yticks(tick_positions)\n",
    "\n",
    "                # Optional: sequence length overlay\n",
    "                seq_len = len(sequences[i].replace(\"-\", \"\"))\n",
    "                ax.axvline(seq_len, color='white', linestyle='--', linewidth=1)\n",
    "                ax.axhline(seq_len, color='white', linestyle='--', linewidth=1)\n",
    "\n",
    "                plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "                plt.tight_layout()\n",
    "\n",
    "                # Save to TensorBoard\n",
    "                buf = io.BytesIO()\n",
    "                plt.savefig(buf, format='png')\n",
    "                plt.close(fig)\n",
    "                buf.seek(0)\n",
    "\n",
    "                image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "                image = tf.expand_dims(image, 0)\n",
    "                tf.summary.image(f\"AttentionMaps/Sample{i}_Head{h}\", image, step=step)\n",
    "\n",
    "            tf.summary.text(f\"Attention/Sequence_{i}\", tf.convert_to_tensor([sequences[i]]), step=step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7c89612-ede3-462c-b597-53d4f818dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ESM eval setup (PyTorch) ---\n",
    "import torch, numpy as np, matplotlib.pyplot as plt\n",
    "from scipy.linalg import sqrtm\n",
    "import umap\n",
    "import esm\n",
    "\n",
    "# Config knobs (can be FLAGS if you prefer)\n",
    "ESM_MODEL_NAME = getattr(FLAGS, \"esm_model\", \"esm2_t6_8M_UR50D\")  # small & fast\n",
    "ESM_DEVICE     = getattr(FLAGS, \"esm_device\", \"cpu\")              # \"cpu\" is safe with TF; use \"cuda\" if you want\n",
    "ESM_BATCH      = getattr(FLAGS, \"esm_batch\", 64)\n",
    "ESM_EVAL_N     = getattr(FLAGS, \"esm_eval_n\", 128)                # sequences per class (real/gen) per eval\n",
    "\n",
    "# Load ESM once\n",
    "def _load_esm(name: str):\n",
    "    name = name.strip()\n",
    "    if name == \"esm2_t6_8M_UR50D\":\n",
    "        m, A = esm.pretrained.esm2_t6_8M_UR50D(); layer = 6\n",
    "    elif name == \"esm2_t12_35M_UR50D\":\n",
    "        m, A = esm.pretrained.esm2_t12_35M_UR50D(); layer = 12\n",
    "    else:\n",
    "        m, A = esm.pretrained.esm2_t6_8M_UR50D(); layer = 6\n",
    "    return m.eval(), A, layer\n",
    "\n",
    "_ESM_MODEL, _ESM_ALPH, _ESM_LAYER = _load_esm(ESM_MODEL_NAME)\n",
    "\n",
    "# mean-pooled ESM embeddings from sequences (list[str])\n",
    "def esm_embed(seqs, model=_ESM_MODEL, alphabet=_ESM_ALPH, layer=_ESM_LAYER,\n",
    "              batch_size=ESM_BATCH, device=ESM_DEVICE):\n",
    "    model = model.to(device)\n",
    "    bc = alphabet.get_batch_converter()\n",
    "    vecs = []\n",
    "    for i in range(0, len(seqs), batch_size):\n",
    "        chunk = seqs[i:i+batch_size]\n",
    "        labels, strs, toks = bc([(\"seq\", s) for s in chunk])\n",
    "        toks = toks.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(toks, repr_layers=[layer])\n",
    "            rep = out[\"representations\"][layer][:, 1:-1, :].mean(1)  # drop CLS/EOS, mean over tokens\n",
    "        vecs.append(rep.detach().cpu().numpy())\n",
    "    return np.vstack(vecs) if vecs else np.zeros((0, 0), dtype=np.float32)\n",
    "\n",
    "def frechet(mu1, C1, mu2, C2, eps=1e-6):\n",
    "    C1 = C1 + np.eye(C1.shape[0]) * eps\n",
    "    C2 = C2 + np.eye(C2.shape[0]) * eps\n",
    "    diff = (mu1 - mu2)\n",
    "    covmean = sqrtm(C1.dot(C2))\n",
    "    if np.iscomplexobj(covmean): covmean = covmean.real\n",
    "    return float(diff.dot(diff) + np.trace(C1 + C2 - 2 * covmean))\n",
    "\n",
    "def esm_fid(E_real, E_gen):\n",
    "    if len(E_real)==0 or len(E_gen)==0: return np.nan\n",
    "    mu_r, C_r = E_real.mean(0), np.cov(E_real, rowvar=False)\n",
    "    mu_g, C_g = E_gen.mean(0), np.cov(E_gen, rowvar=False)\n",
    "    return frechet(mu_r, C_r, mu_g, C_g)\n",
    "\n",
    "def _fig_to_img(fig, close=True):\n",
    "    fig.canvas.draw()\n",
    "    w, h = fig.canvas.get_width_height()\n",
    "    img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8).reshape(h, w, 3)\n",
    "    if close: plt.close(fig)\n",
    "    return img\n",
    "\n",
    "def esm_umap_image(E_real, E_gen, title=\"UMAP (ESM) Real vs Gen\"):\n",
    "    if len(E_real)==0 or len(E_gen)==0:\n",
    "        return None\n",
    "    mapper = umap.UMAP(n_neighbors=30, min_dist=0.1, metric=\"cosine\", random_state=0)\n",
    "    Zr = mapper.fit_transform(E_real)\n",
    "    Zg = mapper.transform(E_gen)\n",
    "    fig = plt.figure(figsize=(5, 4))\n",
    "    plt.scatter(Zr[:,0], Zr[:,1], s=6, alpha=0.30, label=\"Real\")\n",
    "    plt.scatter(Zg[:,0], Zg[:,1], s=8, alpha=0.70, label=\"Gen\")\n",
    "    plt.legend(loc=\"best\"); plt.title(title); plt.tight_layout()\n",
    "    return _fig_to_img(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f541a05-4250-4915-a23b-3ae14921f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_generated_sequences(g_model, z_dim, decode_batch_fn, target_n, per_call=128):\n",
    "    seqs = []\n",
    "    while len(seqs) < target_n:\n",
    "        z = tf.random.normal([min(per_call, target_n - len(seqs)), z_dim], dtype=tf.float32)\n",
    "        hard = g_model(z, training=False, return_hard=True)\n",
    "        hard = tf.squeeze(hard, axis=1)\n",
    "        seqs.extend(decode_batch_fn(hard))\n",
    "    return seqs[:target_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd861ca5-27dd-4508-ab4f-2e787458b14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "real_df = pd.read_csv(\"/home1/zzhang201@kgi.edu/GAN/zzGAN/gan/data/zz/sample_seqs.csv\")\n",
    "real_seqs = real_df[\"sequence\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac1f7f2a-fdbc-496a-aa16-272f999031ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Diagnostics helpers ---\n",
    "import os, numpy as np\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "def current_lr(opt):\n",
    "    \"\"\"Return a Python float for the optimizer LR regardless of schedule/variable/float.\"\"\"\n",
    "    lr = getattr(opt, \"lr\", None) or getattr(opt, \"learning_rate\", None)\n",
    "    try:\n",
    "        return float(tf.keras.backend.get_value(lr))\n",
    "    except Exception:\n",
    "        # e.g., schedules\n",
    "        return float(tf.keras.backend.get_value(lr(tf.cast(model.g_model.global_step, tf.float32))))\n",
    "\n",
    "def _count_missing_grads_t(grads):\n",
    "    # Tensor int32 count of None grads (safe inside @tf.function)\n",
    "    return tf.reduce_sum(tf.constant([1 if g is None else 0 for g in grads], dtype=tf.int32))\n",
    "\n",
    "for l in model.g_model.layers:\n",
    "    if hasattr(l, \"disable_sn\"):\n",
    "        l.disable_sn.assign(False)   # True to disable sn for debugging or False to re-enable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d22fa27a-5148-42b7-8f6b-b39ee255a729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/zzhang201@kgi.edu/.conda/envs/tf2_16/lib/python3.9/site-packages/scipy/spatial/distance.py:1262: RuntimeWarning: invalid value encountered in divide\n",
      "  q = q / np.sum(q, axis=axis, keepdims=True)\n",
      "/home1/zzhang201@kgi.edu/.conda/envs/tf2_16/lib/python3.9/site-packages/scipy/spatial/distance.py:1261: RuntimeWarning: invalid value encountered in divide\n",
      "  p = p / np.sum(p, axis=axis, keepdims=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: YREGEGV-Y-EGEGV-Y-EGEGV-Y-EGEGV-Y-EREGV-Y-EGEGV-Y-...\n",
      "Sample 1: YREGEGV-Y-EGEGV-Y-EGEGV-Y-EGEGV-Y-EGEGV-Y-EGEGV-Y-...\n",
      "Sample 2: YREGEGV-Y-EGEGV-Y-EREGV-Y-EGEGV-Y-EGEGV-Y-EGEGV-Y-...\n",
      "Sample 3: YREGEGV-Y-EGEGV-Y-EGEGV-V-EGEGV-Y-EGEGV-Y-EGEGV-Y-...\n",
      "Sample 4: VREGEGV-Y-EGEGV-Y-EGEGV-Y-EGEGV-Y-EREGV-Y-EGEGV-Y-...\n",
      "Sample 5: YREGEGV-Y-EGEGV-Y-EGEGV-Y-EGEGV-Y-EREGV-Y-EGEGV-Y-...\n",
      "Sample 6: V-EGEGV-Y-EGEGV-Y-EREGV-Y-EGEGV-Y-EGEGV-Y-EGEGV-Y-...\n",
      "Sample 7: Y-EGEGV-Y-EGEGV-Y-EGEGV-Y-EGEGV-Y-EGEGV-Y-EGEGV-Y-...\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/zzhang201@kgi.edu/.conda/envs/tf2_16/lib/python3.9/site-packages/scipy/spatial/distance.py:1262: RuntimeWarning: invalid value encountered in divide\n",
      "  q = q / np.sum(q, axis=axis, keepdims=True)\n",
      "/home1/zzhang201@kgi.edu/.conda/envs/tf2_16/lib/python3.9/site-packages/scipy/spatial/distance.py:1261: RuntimeWarning: invalid value encountered in divide\n",
      "  p = p / np.sum(p, axis=axis, keepdims=True)\n",
      "/home1/zzhang201@kgi.edu/.conda/envs/tf2_16/lib/python3.9/site-packages/scipy/spatial/distance.py:1262: RuntimeWarning: invalid value encountered in divide\n",
      "  q = q / np.sum(q, axis=axis, keepdims=True)\n",
      "/home1/zzhang201@kgi.edu/.conda/envs/tf2_16/lib/python3.9/site-packages/scipy/spatial/distance.py:1261: RuntimeWarning: invalid value encountered in divide\n",
      "  p = p / np.sum(p, axis=axis, keepdims=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: AG-G-GEGQG-G-GEGQG-G-GEGQG-G-GEGQG-G-GEGQG-G-GEGQG...\n",
      "Sample 1: -G-G-GEGQG-G-GEGQG-G-GEGQG-G-GEGQG-G-GEGQG-G-GEGQG...\n",
      "Sample 2: -G-G-GEGQG-G-GEGQG-G-GEGQG-G-GEGQG-G-GEGQG-G-GEGQG...\n",
      "Sample 3: AG-G-GEGQG-G-GEGQG-G-GEGQG-G-GEGQG-G-GEGQG-G-GEGQG...\n",
      "Sample 4: AG-G-GEGQG-G-GEGQG-G-GEGQG-G-GEGQG-G-GEGQG-G-GEGQG...\n",
      "Sample 5: AG-G-GEGQG-G-GEGQG-G-GEGQG-G-GEGQG-G-GEGQG-G-GEGQG...\n",
      "Sample 6: AG-G-GEGQG-G-GEGQG-G-GEGQG-G-GEGQG-G-GEGQG-G-GEGQG...\n",
      "Sample 7: AG-G-GEGQG-G-GEGQG-G-GEGQG-G-GEGQG-G-GEGQG-G-GEGQG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/zzhang201@kgi.edu/.conda/envs/tf2_16/lib/python3.9/site-packages/scipy/spatial/distance.py:1262: RuntimeWarning: invalid value encountered in divide\n",
      "  q = q / np.sum(q, axis=axis, keepdims=True)\n",
      "/home1/zzhang201@kgi.edu/.conda/envs/tf2_16/lib/python3.9/site-packages/scipy/spatial/distance.py:1261: RuntimeWarning: invalid value encountered in divide\n",
      "  p = p / np.sum(p, axis=axis, keepdims=True)\n",
      "/home1/zzhang201@kgi.edu/.conda/envs/tf2_16/lib/python3.9/site-packages/scipy/spatial/distance.py:1262: RuntimeWarning: invalid value encountered in divide\n",
      "  q = q / np.sum(q, axis=axis, keepdims=True)\n",
      "/home1/zzhang201@kgi.edu/.conda/envs/tf2_16/lib/python3.9/site-packages/scipy/spatial/distance.py:1261: RuntimeWarning: invalid value encountered in divide\n",
      "  p = p / np.sum(p, axis=axis, keepdims=True)\n",
      "/home1/zzhang201@kgi.edu/.conda/envs/tf2_16/lib/python3.9/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "/tmp/SLURM_18190/ipykernel_1337929/1700971630.py:59: MatplotlibDeprecationWarning: The tostring_rgb function was deprecated in Matplotlib 3.8 and will be removed in 3.10. Use buffer_rgba instead.\n",
      "  img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8).reshape(h, w, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: --E-QLQA--A-VRIA--E-QYQA--A-VRIA--E-QYQAG-A-VRIA--...\n",
      "Sample 1: --E-QLQA--A-VRIA--E-QYQAG-A-VRIA--E-QLQAG-A-VRIA--...\n",
      "Sample 2: --E-QLQA--A-VRIA--E-QLQAG-A-VRIA--E-QYQAG-A-VRIA--...\n",
      "Sample 3: --E-QLQA--A-VRIA--E-QLQA--A-VRIA--E-QYQAG-A-VRIA--...\n",
      "Sample 4: --E-QLQA--A-VRIA--E-QYQAG-A-VRIA--E-QLQAG-A-VRIA--...\n",
      "Sample 5: --E-QLQA--A-VRIA--E-QLQAG---VRIA--E-QYQAG-A-VRIA--...\n",
      "Sample 6: --E-QLQA--A-VRIA--E-QYQAG-A-VRIA--E-QYQAG-A-VRIA--...\n",
      "Sample 7: --E-QLQA--A-VRIA--E-QYQAG-A-VRIA--E-QYQAG-A-VRIA--...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/zzhang201@kgi.edu/.conda/envs/tf2_16/lib/python3.9/site-packages/scipy/spatial/distance.py:1262: RuntimeWarning: invalid value encountered in divide\n",
      "  q = q / np.sum(q, axis=axis, keepdims=True)\n",
      "/home1/zzhang201@kgi.edu/.conda/envs/tf2_16/lib/python3.9/site-packages/scipy/spatial/distance.py:1261: RuntimeWarning: invalid value encountered in divide\n",
      "  p = p / np.sum(p, axis=axis, keepdims=True)\n",
      "/home1/zzhang201@kgi.edu/.conda/envs/tf2_16/lib/python3.9/site-packages/scipy/spatial/distance.py:1262: RuntimeWarning: invalid value encountered in divide\n",
      "  q = q / np.sum(q, axis=axis, keepdims=True)\n",
      "/home1/zzhang201@kgi.edu/.conda/envs/tf2_16/lib/python3.9/site-packages/scipy/spatial/distance.py:1261: RuntimeWarning: invalid value encountered in divide\n",
      "  p = p / np.sum(p, axis=axis, keepdims=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: ---LNT-STRDRA-A-SRILNT-STRDRA-A-SRILNT-STRDRA-A-SR...\n",
      "Sample 1: ---LNT-STRDRA-A-SRILNT-STRDRA-A-SRILNT-STRDRA-A-SR...\n",
      "Sample 2: ---LNT-STRDRA-A-SRILNT-STRDRA-A-SRILNT-STRDRA-A-SR...\n",
      "Sample 3: ---LNT-STRDRA-A-SRILNT-STRDRA-A-SRILNT-STRDRA-A-SR...\n",
      "Sample 4: ---LNT-STRDRA-A-SRILNT-STRDRA-A-SRILNT-STRDRA-A-SR...\n",
      "Sample 5: ---LNT-STRDRA-A-SRILNT-STRDRA-A-SRILNT-STRDRA-A-SR...\n",
      "Sample 6: ---LNT-STRDRA-A-SRILNT-STRDRA-A-SRILNT-STRDRA-A-SR...\n",
      "Sample 7: ---LNT-STRDRA-A-SRILNT-STRDRA-A-SRILNT-STRDRA-A-SR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/zzhang201@kgi.edu/.conda/envs/tf2_16/lib/python3.9/site-packages/scipy/spatial/distance.py:1262: RuntimeWarning: invalid value encountered in divide\n",
      "  q = q / np.sum(q, axis=axis, keepdims=True)\n",
      "/home1/zzhang201@kgi.edu/.conda/envs/tf2_16/lib/python3.9/site-packages/scipy/spatial/distance.py:1261: RuntimeWarning: invalid value encountered in divide\n",
      "  p = p / np.sum(p, axis=axis, keepdims=True)\n",
      "/home1/zzhang201@kgi.edu/.conda/envs/tf2_16/lib/python3.9/site-packages/scipy/spatial/distance.py:1262: RuntimeWarning: invalid value encountered in divide\n",
      "  q = q / np.sum(q, axis=axis, keepdims=True)\n",
      "/home1/zzhang201@kgi.edu/.conda/envs/tf2_16/lib/python3.9/site-packages/scipy/spatial/distance.py:1261: RuntimeWarning: invalid value encountered in divide\n",
      "  p = p / np.sum(p, axis=axis, keepdims=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: G-YVQLQVG-LVQTATSGG----VG-LVQTATSGG----VG-LVQTATSG...\n",
      "Sample 1: G-YVQLQVG-LVQTATSGF----VG-LVQTATSGN----VG-LVQTATSG...\n",
      "Sample 2: G-YVQLQVG-LVQTATSGG----VG-LVQTATSGG----VG-LVQTATSG...\n",
      "Sample 3: G-YVQLQVG-LVQTATSGG----VG-LVQTATSGN----VG-LVQTATSG...\n",
      "Sample 4: G-YVQLQVG-LVQTATSGG----VG-LVQTATSGG----VG-LVQTATSG...\n",
      "Sample 5: G-YVQLQVG-LVQTATSGG----VG-LVQTATSGN----VG-LVQTATSG...\n",
      "Sample 6: G-YVQLQVG-LVQTATSGG----VG-LVQTATSGF----VG-LVQTATSG...\n",
      "Sample 7: G-YVQLQVG-LVQTATSGF----VG-LVQTATSGG----VG-LVQTATSG...\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n",
      "Limiting hmmer search to species ['human', 'mouse'] was requested but hits did not achieve a high enough bitscore. Reverting to using any species\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/zzhang201@kgi.edu/.conda/envs/tf2_16/lib/python3.9/site-packages/scipy/spatial/distance.py:1262: RuntimeWarning: invalid value encountered in divide\n",
      "  q = q / np.sum(q, axis=axis, keepdims=True)\n",
      "/home1/zzhang201@kgi.edu/.conda/envs/tf2_16/lib/python3.9/site-packages/scipy/spatial/distance.py:1261: RuntimeWarning: invalid value encountered in divide\n",
      "  p = p / np.sum(p, axis=axis, keepdims=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at step 4999: /home1/zzhang201@kgi.edu/GAN/zzGAN/logs/zz/trial2/20251006-021055/checkpoints/ckpt-1\n",
      "e.g. first few keys: ['_CHECKPOINTABLE_OBJECT_GRAPH', 'd_optimizer/_iterations/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/_learning_rate/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/dynamic_scale/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/inner_optimizer/_iterations/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/inner_optimizer/_learning_rate/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/inner_optimizer/_variables/10/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/inner_optimizer/_variables/11/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/inner_optimizer/_variables/12/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/inner_optimizer/_variables/13/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/inner_optimizer/_variables/14/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/inner_optimizer/_variables/15/.ATTRIBUTES/VARIABLE_VALUE']\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "res_block_0 not saved — ensure setattr(...) and that a forward pass happened before saving.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 123\u001b[0m\n\u001b[1;32m    121\u001b[0m     keys \u001b[38;5;241m=\u001b[39m [n \u001b[38;5;28;01mfor\u001b[39;00m n, _ \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mlist_variables(latest)]\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me.g. first few keys:\u001b[39m\u001b[38;5;124m\"\u001b[39m, keys[:\u001b[38;5;241m12\u001b[39m])\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator/res_block_0/deconv\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m keys), \\\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mres_block_0 not saved — ensure setattr(...) and that a forward pass happened before saving.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# === ESM eval (EMA), every 2500 steps ===\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2500\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m step \u001b[38;5;241m==\u001b[39m FLAGS\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Assemble fresh real/gen sets here so variables exist\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# (don’t rely on `real_seqs` from another block)\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# Real:\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: res_block_0 not saved — ensure setattr(...) and that a forward pass happened before saving."
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "real_iter = iter(dataset)\n",
    "d_prev_k = None                       # for kernel delta probe\n",
    "ESM_EVAL_N = 512                      # adjust if you want more/less for FID\n",
    "\n",
    "# === Training Loop ===\n",
    "for step in range(FLAGS.steps):\n",
    "    # --- D:G ratio schedule ---\n",
    "    d_step = 1 if step < FLAGS.multid_schedule else FLAGS.d_step\n",
    "\n",
    "    # --- Discriminator steps ---\n",
    "    for _ in range(d_step):\n",
    "        real_batch = next(real_iter)\n",
    "        d_loss, d_grad_norm, d_wgan, d_gp, d_missing, d_real_m, d_fake_m = train_discriminator_step(\n",
    "            model.g_model, model.d_model, model.d_optim, real_batch, FLAGS.z_dim\n",
    "        )\n",
    "\n",
    "    # --- Generator step ---\n",
    "    g_loss, g_grad_norm, g_missing, g_entropy = train_generator_step(\n",
    "        model.g_model, model.d_model, model.g_optim, FLAGS.z_dim\n",
    "    )\n",
    "\n",
    "    model.global_step.assign_add(1)\n",
    "\n",
    "    # === Light logging (every 100 steps) ===\n",
    "    if (step + 1) % 100 == 0 or step == FLAGS.steps - 1:\n",
    "        with summary_writer.as_default():\n",
    "            # D losses\n",
    "            tf.summary.scalar(\"D/total\",     d_loss,    step=step)\n",
    "            tf.summary.scalar(\"D/wgan\",      d_wgan,    step=step)\n",
    "            tf.summary.scalar(\"D/gp\",        d_gp,      step=step)\n",
    "            # G losses\n",
    "            tf.summary.scalar(\"G/total\",     g_loss,    step=step)\n",
    "            tf.summary.scalar(\"G/entropy\",   g_entropy, step=step)\n",
    "            # Grad norms & missing counts\n",
    "            tf.summary.scalar(\"D/grad_norm\", d_grad_norm, step=step)\n",
    "            tf.summary.scalar(\"G/grad_norm\", g_grad_norm, step=step)\n",
    "            tf.summary.scalar(\"D/missing_grads\", d_missing, step=step)\n",
    "            tf.summary.scalar(\"G/missing_grads\", g_missing, step=step)\n",
    "            # LRs\n",
    "            tf.summary.scalar(\"LR/D\", current_lr(model.d_optim), step=step)\n",
    "            tf.summary.scalar(\"LR/G\", current_lr(model.g_optim), step=step)\n",
    "            # D logits means\n",
    "            tf.summary.scalar(\"D/real_logit_mean\", d_real_m, step=step)\n",
    "            tf.summary.scalar(\"D/fake_logit_mean\", d_fake_m, step=step)\n",
    "\n",
    "    # === Heavier probes (every 1000 steps) ===\n",
    "    if (step + 1) % 1000 == 0 or step == FLAGS.steps - 1:\n",
    "        try:\n",
    "            k = model.d_model.get_layer(\"conv1\").kernel\n",
    "            delta = tf.constant(0.0, tf.float32)\n",
    "            if d_prev_k is not None:\n",
    "                delta = tf.norm(tf.cast(k, tf.float32) - tf.cast(d_prev_k, tf.float32))\n",
    "            d_prev_k = tf.identity(k)  # snapshot for next time\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar(\"D/conv1_kernel_delta\", delta, step=step)\n",
    "        except Exception:\n",
    "            pass\n",
    "        clear_output(wait=True)\n",
    "        # Generated sequence preview\n",
    "        z = tf.random.normal([8, FLAGS.z_dim], dtype=tf.float32)\n",
    "        samples = model.g_model(z, training=False, return_hard=True, return_attention=True)\n",
    "        samples = tf.squeeze(samples, axis=1)\n",
    "        sequences = [decode_sequence(seq) for seq in samples]\n",
    "        append_generated_sequences(step=step, sequences=sequences)\n",
    "\n",
    "        attn_scores = model.g_model.last_attn_scores\n",
    "        save_attention_to_disk(attn_scores, sequences, step, output_dir=attn_dir)\n",
    "        tf.print(\"\\n\".join(f\"Sample {i}: {sequences[i]}...\" for i in range(len(sequences))))\n",
    "\n",
    "    # === Quality metrics (every 500 steps) ===\n",
    "    if (step + 1) % 1000 == 0 or step == FLAGS.steps - 1:\n",
    "        # fresh batches for metrics (don’t rely on prior scope variables)\n",
    "        z = tf.random.normal([64, FLAGS.z_dim], dtype=tf.float32)\n",
    "        hard_batch = model.g_model(z, training=False, return_hard=True)\n",
    "        real_batch = next(real_iter)\n",
    "\n",
    "        gen_seqs  = decode_batch(hard_batch)\n",
    "        real_seqs = decode_batch(real_batch)\n",
    "\n",
    "        fr_loss, cdr_loss = quality_losses(gen_seqs, real_seqs, full_real_frs_csv=\"data/vh_regions.csv\")\n",
    "        anarci_stats = anarci_quality_log(gen_seqs)\n",
    "\n",
    "        p_gen  = get_aa_distribution(hard_batch)\n",
    "        p_real = get_aa_distribution(real_batch)\n",
    "        jsd    = jensenshannon(p_real, p_gen, base=2)\n",
    "\n",
    "        real_lengths = sequence_lengths(real_batch)\n",
    "        gen_lengths  = sequence_lengths(hard_batch)\n",
    "        jsd_pos      = js_divergence_per_position(real_batch, hard_batch)\n",
    "        valid_jsd    = jsd_pos[~np.isnan(jsd_pos)]\n",
    "        length_diff  = tf.reduce_mean(tf.cast(gen_lengths, tf.float32)) - tf.reduce_mean(tf.cast(real_lengths, tf.float32))\n",
    "\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar(\"JS_Divergence/Aggregated\", jsd, step=step)\n",
    "            tf.summary.histogram(\"JS_Divergence/Per_Position\", jsd_pos, step=step)\n",
    "            tf.summary.scalar(\"JS_Divergence/Per_Position_Mean\", float(np.mean(valid_jsd)), step=step)\n",
    "            tf.summary.scalar(\"JS_Divergence/Per_Position_Max\",  float(np.max(valid_jsd)),  step=step)\n",
    "            tf.summary.scalar(\"JS_Divergence/Per_Position_Std\",  float(np.std(valid_jsd)),  step=step)\n",
    "\n",
    "            tf.summary.scalar(\"Length/Real_mean\",      tf.reduce_mean(real_lengths), step=step)\n",
    "            tf.summary.scalar(\"Length/Generated_mean\", tf.reduce_mean(gen_lengths),  step=step)\n",
    "            tf.summary.scalar(\"Length/Generated_stddev\", tf.math.reduce_std(tf.cast(gen_lengths, tf.float32)), step=step)\n",
    "            tf.summary.scalar(\"Length/Diff_Gen_minus_Real\", length_diff, step=step)\n",
    "\n",
    "            tf.summary.scalar(\"Anarci/FR_Loss\",      fr_loss, step=step)\n",
    "            tf.summary.scalar(\"Anarci/CDR_JS_Loss\",  cdr_loss, step=step)\n",
    "            tf.summary.scalar(\"Anarci/AnyHit\",       anarci_stats[\"full_hit\"] + anarci_stats[\"partial_hit\"], step=step)\n",
    "            tf.summary.scalar(\"Anarci/FullHit\",      anarci_stats[\"full_hit\"],     step=step)\n",
    "            tf.summary.scalar(\"Anarci/PartialHit\",   anarci_stats[\"partial_hit\"],  step=step)\n",
    "            tf.summary.scalar(\"Anarci/NoHit\",        anarci_stats[\"no_hit\"],       step=step)\n",
    "\n",
    "    # === Checkpointing ===\n",
    "    if (step + 1) % FLAGS.save_checkpoint_sec == 0 or step == FLAGS.steps - 1:\n",
    "        path = ckpt_manager.save()\n",
    "        print(f\"Checkpoint saved at step {step}: {path}\")\n",
    "\n",
    "        # one-time sanity (can disable later)\n",
    "        latest = tf.train.latest_checkpoint(CKPT_DIR)\n",
    "        keys = [n for n, _ in tf.train.list_variables(latest)]\n",
    "        print(\"e.g. first few keys:\", keys[:12])\n",
    "        assert any(\"generator/res_block_0/deconv\" in k for k in keys), \\\n",
    "            \"res_block_0 not saved — ensure setattr(...) and that a forward pass happened before saving.\"\n",
    "\n",
    "    # === ESM eval (EMA), every 2500 steps ===\n",
    "    if (step + 1) % 2500 == 0 or step == FLAGS.steps - 1:\n",
    "        # Assemble fresh real/gen sets here so variables exist\n",
    "        # (don’t rely on `real_seqs` from another block)\n",
    "        # Real:\n",
    "        real_eval_batch = next(real_iter)\n",
    "        real_eval_seqs  = decode_batch(real_eval_batch)\n",
    "        # Gen (EMA weights):\n",
    "        with ema.average_parameters(model.g_model):\n",
    "            gen_eval_seqs = sample_generated_sequences(model.g_model, FLAGS.z_dim, decode_batch, ESM_EVAL_N)\n",
    "\n",
    "        E_r = esm_embed(real_eval_seqs)\n",
    "        E_g = esm_embed(gen_eval_seqs)\n",
    "        fid_esm = esm_fid(E_r, E_g)\n",
    "        umap_img = esm_umap_image(E_r, E_g, title=\"UMAP (ESM) Real vs Gen (EMA)\")\n",
    "\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar(\"ESM/FID_EMA\", fid_esm, step=step)\n",
    "            if umap_img is not None:\n",
    "                tf.summary.image(\"UMAP_ESM/Real_vs_Gen_EMA\", umap_img[None, ...], step=step)\n",
    "\n",
    "    # === Finalization ===\n",
    "    if step == FLAGS.steps - 1:\n",
    "        with ema.average_parameters(model.g_model):\n",
    "            z = tf.random.normal([300, FLAGS.z_dim], dtype=tf.float32)\n",
    "            hard_batch = model.g_model(z, training=False, return_hard=True, return_attention=True)\n",
    "            hard_batch = tf.squeeze(hard_batch, axis=1)\n",
    "            sequences = [decode_sequence(seq) for seq in hard_batch]\n",
    "\n",
    "            diverse_seqs = filter_diverse_sequences(sequences, threshold=0.05, max_count=None)\n",
    "\n",
    "            # save a few attention maps\n",
    "            for i in range(min(4, len(sequences))):\n",
    "                for h in range(2):\n",
    "                    if model.g_model.last_attn_scores is not None:\n",
    "                        attn = model.g_model.last_attn_scores[i, h]  # [L, L]\n",
    "                        save_attention_png(attn.numpy(), filename=f\"attn_sample{i}_head{h}.png\", aa_seq=sequences[i])\n",
    "\n",
    "            with summary_writer.as_default():\n",
    "                for i, seq in enumerate(diverse_seqs[:30]):\n",
    "                    tf.summary.text(f\"Final/DiverseSequence_{i}\", tf.convert_to_tensor([seq]), step=step)\n",
    "                tf.summary.scalar(\"Final/NumDiverseSequences\", len(diverse_seqs), step=step)\n",
    "\n",
    "            print(f\"Generated {len(sequences)} sequences.\")\n",
    "            print(f\"Found {len(diverse_seqs)} diverse sequences.\")\n",
    "\n",
    "        summary_writer.flush()\n",
    "        generate_dynamic_layout(logdir=os.path.join(FLAGS.logdir, FLAGS.name), run_name=timestamp)\n",
    "        print(f\"Layout updated based on {log_dir}\")\n",
    "\n",
    "        final_path = ckpt_manager.save()\n",
    "        print(\"Final checkpoint saved:\", final_path)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a34c895-b850-4155-86e0-63e6226888e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e.g. first few keys: ['_CHECKPOINTABLE_OBJECT_GRAPH', 'd_optimizer/_iterations/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/_learning_rate/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/dynamic_scale/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/inner_optimizer/_iterations/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/inner_optimizer/_learning_rate/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/inner_optimizer/_variables/10/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/inner_optimizer/_variables/11/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/inner_optimizer/_variables/12/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/inner_optimizer/_variables/13/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/inner_optimizer/_variables/14/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/inner_optimizer/_variables/15/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/inner_optimizer/_variables/16/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/inner_optimizer/_variables/17/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/inner_optimizer/_variables/2/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/inner_optimizer/_variables/3/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/inner_optimizer/_variables/4/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/inner_optimizer/_variables/5/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/inner_optimizer/_variables/6/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/inner_optimizer/_variables/7/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/inner_optimizer/_variables/8/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/inner_optimizer/_variables/9/.ATTRIBUTES/VARIABLE_VALUE', 'd_optimizer/step_counter/.ATTRIBUTES/VARIABLE_VALUE', 'discriminator/conv1/_kernel/.ATTRIBUTES/VARIABLE_VALUE', 'discriminator/conv1/bias/.ATTRIBUTES/VARIABLE_VALUE', 'discriminator/conv2/_kernel/.ATTRIBUTES/VARIABLE_VALUE', 'discriminator/conv2/bias/.ATTRIBUTES/VARIABLE_VALUE', 'discriminator/conv3/_kernel/.ATTRIBUTES/VARIABLE_VALUE', 'discriminator/conv3/bias/.ATTRIBUTES/VARIABLE_VALUE', 'discriminator/dense/_kernel/.ATTRIBUTES/VARIABLE_VALUE', 'discriminator/dense/bias/.ATTRIBUTES/VARIABLE_VALUE', 'ema/pairs/10/1/.ATTRIBUTES/VARIABLE_VALUE', 'ema/pairs/11/1/.ATTRIBUTES/VARIABLE_VALUE', 'ema/pairs/12/1/.ATTRIBUTES/VARIABLE_VALUE', 'ema/pairs/13/1/.ATTRIBUTES/VARIABLE_VALUE', 'ema/pairs/14/1/.ATTRIBUTES/VARIABLE_VALUE', 'ema/pairs/15/1/.ATTRIBUTES/VARIABLE_VALUE', 'ema/pairs/16/1/.ATTRIBUTES/VARIABLE_VALUE', 'ema/pairs/17/1/.ATTRIBUTES/VARIABLE_VALUE', 'ema/pairs/18/1/.ATTRIBUTES/VARIABLE_VALUE', 'ema/pairs/2/1/.ATTRIBUTES/VARIABLE_VALUE', 'ema/pairs/20/1/.ATTRIBUTES/VARIABLE_VALUE', 'ema/pairs/21/1/.ATTRIBUTES/VARIABLE_VALUE', 'ema/pairs/22/1/.ATTRIBUTES/VARIABLE_VALUE', 'ema/pairs/23/1/.ATTRIBUTES/VARIABLE_VALUE', 'ema/pairs/24/1/.ATTRIBUTES/VARIABLE_VALUE', 'ema/pairs/25/1/.ATTRIBUTES/VARIABLE_VALUE', 'ema/pairs/26/1/.ATTRIBUTES/VARIABLE_VALUE', 'ema/pairs/27/1/.ATTRIBUTES/VARIABLE_VALUE', 'ema/pairs/3/1/.ATTRIBUTES/VARIABLE_VALUE', 'ema/pairs/4/1/.ATTRIBUTES/VARIABLE_VALUE', 'ema/pairs/5/1/.ATTRIBUTES/VARIABLE_VALUE', 'ema/pairs/6/1/.ATTRIBUTES/VARIABLE_VALUE', 'ema/pairs/7/1/.ATTRIBUTES/VARIABLE_VALUE', 'ema/pairs/8/1/.ATTRIBUTES/VARIABLE_VALUE', 'ema/pairs/9/1/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/0/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/1/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/10/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/11/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/12/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/13/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/14/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/15/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/16/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/17/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/18/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/19/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/2/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/20/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/21/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/22/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/23/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/24/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/25/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/26/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/27/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/28/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/29/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/3/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/4/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/5/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/6/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/7/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/8/.ATTRIBUTES/VARIABLE_VALUE', 'ema/shadow_vars/9/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/_iterations/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/_learning_rate/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/dynamic_scale/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_iterations/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_learning_rate/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/10/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/11/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/12/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/13/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/14/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/15/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/16/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/17/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/18/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/19/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/2/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/20/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/21/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/22/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/23/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/24/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/25/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/26/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/27/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/28/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/29/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/3/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/30/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/31/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/32/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/33/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/34/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/35/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/36/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/37/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/38/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/39/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/4/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/40/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/41/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/42/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/43/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/44/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/45/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/46/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/47/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/48/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/49/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/5/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/50/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/51/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/52/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/53/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/54/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/55/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/56/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/57/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/58/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/59/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/6/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/60/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/61/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/7/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/8/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/inner_optimizer/_variables/9/.ATTRIBUTES/VARIABLE_VALUE', 'g_optimizer/step_counter/.ATTRIBUTES/VARIABLE_VALUE', 'generator/attn_ln/beta/.ATTRIBUTES/VARIABLE_VALUE', 'generator/attn_ln/gamma/.ATTRIBUTES/VARIABLE_VALUE', 'generator/final_bn/bn/moving_mean/.ATTRIBUTES/VARIABLE_VALUE', 'generator/final_bn/bn/moving_variance/.ATTRIBUTES/VARIABLE_VALUE', 'generator/last_conv/disable_sn/.ATTRIBUTES/VARIABLE_VALUE', 'generator/last_conv/u/.ATTRIBUTES/VARIABLE_VALUE', 'generator/last_conv/w/.ATTRIBUTES/VARIABLE_VALUE', 'generator/noise_fc/b/.ATTRIBUTES/VARIABLE_VALUE', 'generator/noise_fc/disable_sn/.ATTRIBUTES/VARIABLE_VALUE', 'generator/noise_fc/u/.ATTRIBUTES/VARIABLE_VALUE', 'generator/noise_fc/w/.ATTRIBUTES/VARIABLE_VALUE', 'generator/res_block_0/_functional/_operations/1/disable_sn/.ATTRIBUTES/VARIABLE_VALUE', 'generator/res_block_0/_functional/_operations/1/u/.ATTRIBUTES/VARIABLE_VALUE', 'generator/res_block_0/_functional/_operations/2/moving_mean/.ATTRIBUTES/VARIABLE_VALUE', 'generator/res_block_0/_functional/_operations/2/moving_variance/.ATTRIBUTES/VARIABLE_VALUE', 'generator/res_block_1/_functional/_operations/1/disable_sn/.ATTRIBUTES/VARIABLE_VALUE', 'generator/res_block_1/_functional/_operations/1/u/.ATTRIBUTES/VARIABLE_VALUE', 'generator/res_block_1/_functional/_operations/2/moving_mean/.ATTRIBUTES/VARIABLE_VALUE', 'generator/res_block_1/_functional/_operations/2/moving_variance/.ATTRIBUTES/VARIABLE_VALUE', 'generator/res_block_2/_functional/_operations/1/disable_sn/.ATTRIBUTES/VARIABLE_VALUE', 'generator/res_block_2/_functional/_operations/1/u/.ATTRIBUTES/VARIABLE_VALUE', 'generator/res_block_2/_functional/_operations/2/moving_mean/.ATTRIBUTES/VARIABLE_VALUE', 'generator/res_block_2/_functional/_operations/2/moving_variance/.ATTRIBUTES/VARIABLE_VALUE', 'generator/res_block_3/_functional/_operations/1/disable_sn/.ATTRIBUTES/VARIABLE_VALUE', 'generator/res_block_3/_functional/_operations/1/u/.ATTRIBUTES/VARIABLE_VALUE', 'generator/res_block_3/_functional/_operations/2/moving_mean/.ATTRIBUTES/VARIABLE_VALUE', 'generator/res_block_3/_functional/_operations/2/moving_variance/.ATTRIBUTES/VARIABLE_VALUE', 'generator/res_block_4/_functional/_operations/1/disable_sn/.ATTRIBUTES/VARIABLE_VALUE', 'generator/res_block_4/_functional/_operations/1/u/.ATTRIBUTES/VARIABLE_VALUE', 'generator/res_block_4/_functional/_operations/2/moving_mean/.ATTRIBUTES/VARIABLE_VALUE', 'generator/res_block_4/_functional/_operations/2/moving_variance/.ATTRIBUTES/VARIABLE_VALUE', 'save_counter/.ATTRIBUTES/VARIABLE_VALUE', 'step/.ATTRIBUTES/VARIABLE_VALUE']\n"
     ]
    }
   ],
   "source": [
    "        latest = tf.train.latest_checkpoint(CKPT_DIR)\n",
    "        keys = [n for n, _ in tf.train.list_variables(latest)]\n",
    "        print(\"e.g. first few keys:\", keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "660528e6-20a1-4ecd-9778-700a796e5256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHA keys (first 20): []\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Missing /query_dense/kernel under generator/attn/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 16\u001b[0m\n\u001b[1;32m      9\u001b[0m expected_suffixes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/query_dense/kernel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/query_dense/bias\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/key_dense/kernel\u001b[39m\u001b[38;5;124m\"\u001b[39m,   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/key_dense/bias\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/value_dense/kernel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/value_dense/bias\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/output_dense/kernel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/output_dense/bias\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m ]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m suf \u001b[38;5;129;01min\u001b[39;00m expected_suffixes:\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28many\u001b[39m(k\u001b[38;5;241m.\u001b[39mendswith(suf \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/.ATTRIBUTES/VARIABLE_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator/attn/\u001b[39m\u001b[38;5;124m\"\u001b[39m)), \\\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m under generator/attn/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Found MHA Q/K/V + output weights under generator/attn/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Missing /query_dense/kernel under generator/attn/"
     ]
    }
   ],
   "source": [
    "latest = tf.train.latest_checkpoint(CKPT_DIR)\n",
    "keys = [n for n,_ in tf.train.list_variables(latest)]\n",
    "\n",
    "# Look for MHA sublayers under the attribute name `attn`\n",
    "mha_keys = [k for k in keys if k.startswith(\"generator/attn/\")]\n",
    "print(\"MHA keys (first 20):\", mha_keys[:20])\n",
    "\n",
    "# Typical sublayers in Keras MHA\n",
    "expected_suffixes = [\n",
    "    \"/query_dense/kernel\", \"/query_dense/bias\",\n",
    "    \"/key_dense/kernel\",   \"/key_dense/bias\",\n",
    "    \"/value_dense/kernel\", \"/value_dense/bias\",\n",
    "    \"/output_dense/kernel\",\"/output_dense/bias\",\n",
    "]\n",
    "for suf in expected_suffixes:\n",
    "    assert any(k.endswith(suf + \"/.ATTRIBUTES/VARIABLE_VALUE\") for k in keys if k.startswith(\"generator/attn/\")), \\\n",
    "        f\"Missing {suf} under generator/attn/\"\n",
    "print(\"✓ Found MHA Q/K/V + output weights under generator/attn/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5bb73a02-bd18-49c3-8ee2-8baa10e2f2f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_model.number_of_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8c30cb8-d5ab-4c4c-881f-89bcaa8c6a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHA has weights: 8\n",
      "   kernel (2048, 2, 64)\n",
      "   bias (2, 64)\n",
      "   kernel (2048, 2, 64)\n",
      "   bias (2, 64)\n",
      "   kernel (2048, 2, 64)\n",
      "   bias (2, 64)\n",
      "   kernel (2, 64, 2048)\n",
      "   bias (2048,)\n",
      "Tracked children of generator: ['noise_fc', 'res_block_0', 'res_block_1', 'res_block_2', 'res_block_3', 'res_block_4', 'final_bn', 'last_conv', 'attn', 'attn_ln', 'global_step']\n",
      "Is 'attn' tracked?: True\n"
     ]
    }
   ],
   "source": [
    "# 1) Do we actually have MHA weights in memory?\n",
    "print(\"MHA has weights:\", len(g_model.attn.weights))\n",
    "for w in g_model.attn.weights:\n",
    "    print(\"  \", w.name, w.shape)\n",
    "\n",
    "# 2) Is the attention layer tracked as a child of the generator?\n",
    "deps = getattr(g_model, \"_checkpoint_dependencies\", [])\n",
    "print(\"Tracked children of generator:\",\n",
    "      [ref.name for ref in deps][:50])\n",
    "\n",
    "print(\"Is 'attn' tracked?:\", any(ref.name == \"attn\" for ref in deps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a9fbce1-7c50-4ec5-a170-65fadcdf737a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHA keys found: 0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected 4 kernels & 4 biases, got 0 & 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m num_kernels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(k\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kernel/.ATTRIBUTES/VARIABLE_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m mha_keys)\n\u001b[1;32m     22\u001b[0m num_biases  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(k\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/bias/.ATTRIBUTES/VARIABLE_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m mha_keys)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m num_kernels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m num_biases \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 4 kernels & 4 biases, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_kernels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m & \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_biases\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Found 4×(kernel,bias) for MHA (Q,K,V,Output).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Expected 4 kernels & 4 biases, got 0 & 0"
     ]
    }
   ],
   "source": [
    "import re, tensorflow as tf\n",
    "\n",
    "latest = tf.train.latest_checkpoint(CKPT_DIR)\n",
    "keys = [n for n, _ in tf.train.list_variables(latest)]\n",
    "\n",
    "def find_mha(prefixs=(\"generator/attn/\", \"generator/self_attention/\")):\n",
    "    hits = []\n",
    "    for k in keys:\n",
    "        if any(k.startswith(p) for p in prefixs):\n",
    "            # keep only the actual weight entries\n",
    "            if re.search(r\"/(kernel|bias)/\\.ATTRIBUTES/VARIABLE_VALUE$\", k):\n",
    "                hits.append(k)\n",
    "    return sorted(hits)\n",
    "\n",
    "mha_keys = find_mha()\n",
    "print(\"MHA keys found:\", len(mha_keys))\n",
    "for k in mha_keys:\n",
    "    print(k)\n",
    "\n",
    "# Strong sanity: count kernels & biases\n",
    "num_kernels = sum(k.endswith(\"/kernel/.ATTRIBUTES/VARIABLE_VALUE\") for k in mha_keys)\n",
    "num_biases  = sum(k.endswith(\"/bias/.ATTRIBUTES/VARIABLE_VALUE\") for k in mha_keys)\n",
    "assert num_kernels == 4 and num_biases == 4, f\"Expected 4 kernels & 4 biases, got {num_kernels} & {num_biases}\"\n",
    "print(\"✓ Found 4×(kernel,bias) for MHA (Q,K,V,Output).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32551f7d-9301-475d-b1d5-c463905c9583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python (tf2_16)",
   "language": "python",
   "name": "tf2_16"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
